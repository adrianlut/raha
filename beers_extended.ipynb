{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Data Cleaning Pipeline with Raha and Baran (Minimal and Sequential)\n",
    "We build an end-to-end data cleaning pipeline with our configuration-free error detection and correction systems, Raha and Baran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import IPython.display\n",
    "from pandas import DataFrame, Series\n",
    "import pickle\n",
    "import raha\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Detection with Raha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Instantiating the Detection Class\n",
    "We first instantiate the `Detection` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_1 = raha.Detection()\n",
    "\n",
    "# How many tuples would you label?\n",
    "app_1.LABELING_BUDGET = 20\n",
    "\n",
    "# Would you like to see the logs?\n",
    "app_1.VERBOSE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiating the Dataset\n",
    "We next load and instantiate the dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  index    id            beer_name                           style  \\\n0     1  1436             Pub Beer             American Pale Lager   \n1     2  2265          Devil's Cup         American Pale Ale (APA)   \n2     3  2264  Rise of the Phoenix                    American IPA   \n3     4  2263             Sinister  American Double / Imperial IPA   \n4     5  2262        Sex and Candy                    American IPA   \n\n       ounces    abv  ibu brewery_id               brewery_name  city state  \n0     12.0 oz   0.05  N/A        408  10 Barrel Brewing Company  Bend    OR  \n1    12.0 oz.  0.066  N/A        177        18th Street Brewery  Gary    IN  \n2  12.0 ounce  0.071  N/A        177        18th Street Brewery  Gary    IN  \n3     12.0 oz  0.09%  N/A        177        18th Street Brewery  Gary    IN  \n4    12.0 OZ.  0.075  N/A        177        18th Street Brewery  Gary    IN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>id</th>\n      <th>beer_name</th>\n      <th>style</th>\n      <th>ounces</th>\n      <th>abv</th>\n      <th>ibu</th>\n      <th>brewery_id</th>\n      <th>brewery_name</th>\n      <th>city</th>\n      <th>state</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1436</td>\n      <td>Pub Beer</td>\n      <td>American Pale Lager</td>\n      <td>12.0 oz</td>\n      <td>0.05</td>\n      <td>N/A</td>\n      <td>408</td>\n      <td>10 Barrel Brewing Company</td>\n      <td>Bend</td>\n      <td>OR</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>2265</td>\n      <td>Devil's Cup</td>\n      <td>American Pale Ale (APA)</td>\n      <td>12.0 oz.</td>\n      <td>0.066</td>\n      <td>N/A</td>\n      <td>177</td>\n      <td>18th Street Brewery</td>\n      <td>Gary</td>\n      <td>IN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>2264</td>\n      <td>Rise of the Phoenix</td>\n      <td>American IPA</td>\n      <td>12.0 ounce</td>\n      <td>0.071</td>\n      <td>N/A</td>\n      <td>177</td>\n      <td>18th Street Brewery</td>\n      <td>Gary</td>\n      <td>IN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>2263</td>\n      <td>Sinister</td>\n      <td>American Double / Imperial IPA</td>\n      <td>12.0 oz</td>\n      <td>0.09%</td>\n      <td>N/A</td>\n      <td>177</td>\n      <td>18th Street Brewery</td>\n      <td>Gary</td>\n      <td>IN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>2262</td>\n      <td>Sex and Candy</td>\n      <td>American IPA</td>\n      <td>12.0 OZ.</td>\n      <td>0.075</td>\n      <td>N/A</td>\n      <td>177</td>\n      <td>18th Street Brewery</td>\n      <td>Gary</td>\n      <td>IN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dictionary = {\n",
    "        \"name\": \"beers\",\n",
    "        \"path\": \"datasets/beers/dirty.csv\",\n",
    "        \"clean_path\": \"datasets/beers/clean.csv\"\n",
    "    }\n",
    "d = app_1.initialize_dataset(dataset_dictionary)\n",
    "d.dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Running Error Detection Strategies\n",
    "Raha runs (all or the promising) error detection strategies on the dataset. This step could take a while because all the strategies should be run on the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "511 strategy profiles are collected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I just load strategies' results as they have already been run on the dataset!\n"
     ]
    }
   ],
   "source": [
    "app_1.run_strategies(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generating Features\n",
    "Raha then generates a feature vector for each data cell based on the output of error detection strategies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 Features are generated for column 0.\n",
      "44 Features are generated for column 1.\n",
      "133 Features are generated for column 2.\n",
      "100 Features are generated for column 3.\n",
      "63 Features are generated for column 4.\n",
      "54 Features are generated for column 5.\n",
      "54 Features are generated for column 6.\n",
      "55 Features are generated for column 7.\n",
      "103 Features are generated for column 8.\n",
      "106 Features are generated for column 9.\n",
      "75 Features are generated for column 10.\n"
     ]
    }
   ],
   "source": [
    "app_1.generate_features(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Building Clusters\n",
    "Raha next builds a hierarchical clustering model for our clustering-based sampling approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A hierarchical clustering model is built for column 0.\n",
      "A hierarchical clustering model is built for column 1.\n",
      "A hierarchical clustering model is built for column 2.\n",
      "A hierarchical clustering model is built for column 3.\n",
      "A hierarchical clustering model is built for column 4.\n",
      "A hierarchical clustering model is built for column 5.\n",
      "A hierarchical clustering model is built for column 6.\n",
      "A hierarchical clustering model is built for column 7.\n",
      "A hierarchical clustering model is built for column 8.\n",
      "A hierarchical clustering model is built for column 9.\n",
      "A hierarchical clustering model is built for column 10.\n"
     ]
    }
   ],
   "source": [
    "app_1.build_clusters(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Interactive Tuple Sampling and Labeling\n",
    "Raha then iteratively samples a tuple. We should label data cells of each sampled tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuple 192 is sampled.\n",
      "Tuple 192 is labeled.\n",
      "Tuple 323 is sampled.\n",
      "Tuple 323 is labeled.\n",
      "Tuple 90 is sampled.\n",
      "Tuple 90 is labeled.\n",
      "Tuple 1121 is sampled.\n",
      "Tuple 1121 is labeled.\n",
      "Tuple 1185 is sampled.\n",
      "Tuple 1185 is labeled.\n",
      "Tuple 1666 is sampled.\n",
      "Tuple 1666 is labeled.\n",
      "Tuple 2251 is sampled.\n",
      "Tuple 2251 is labeled.\n",
      "Tuple 2395 is sampled.\n",
      "Tuple 2395 is labeled.\n",
      "Tuple 1039 is sampled.\n",
      "Tuple 1039 is labeled.\n",
      "Tuple 442 is sampled.\n",
      "Tuple 442 is labeled.\n",
      "Tuple 1955 is sampled.\n",
      "Tuple 1955 is labeled.\n",
      "Tuple 388 is sampled.\n",
      "Tuple 388 is labeled.\n",
      "Tuple 1969 is sampled.\n",
      "Tuple 1969 is labeled.\n",
      "Tuple 2394 is sampled.\n",
      "Tuple 2394 is labeled.\n",
      "Tuple 2391 is sampled.\n",
      "Tuple 2391 is labeled.\n",
      "Tuple 1579 is sampled.\n",
      "Tuple 1579 is labeled.\n",
      "Tuple 2260 is sampled.\n",
      "Tuple 2260 is labeled.\n",
      "Tuple 553 is sampled.\n",
      "Tuple 553 is labeled.\n",
      "Tuple 2221 is sampled.\n",
      "Tuple 2221 is labeled.\n",
      "Tuple 1000 is sampled.\n",
      "Tuple 1000 is labeled.\n"
     ]
    }
   ],
   "source": [
    "while len(d.labeled_tuples) < app_1.LABELING_BUDGET:\n",
    "    app_1.sample_tuple(d)\n",
    "    if d.has_ground_truth:\n",
    "        app_1.label_with_ground_truth(d)\n",
    "    else:\n",
    "        print(\"Label the dirty cells in the following sampled tuple.\")\n",
    "        sampled_tuple = pandas.DataFrame(data=[d.dataframe.iloc[d.sampled_tuple, :]], columns=d.dataframe.columns)\n",
    "        IPython.display.display(sampled_tuple)\n",
    "        for j in range(d.dataframe.shape[1]):\n",
    "            cell = (d.sampled_tuple, j)\n",
    "            value = d.dataframe.iloc[cell]\n",
    "            correction = input(\"What is the correction for value '{}'? Type in the same value if it is not erronous.\\n\".format(value))\n",
    "            user_label = 1 if value != correction else 0\n",
    "            d.labeled_cells[cell] = [user_label, correction]\n",
    "        d.labeled_tuples[d.sampled_tuple] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Propagating User Labels\n",
    "Raha then propagates each user label through its cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of labeled data cells increased from 220 to 24548.\n"
     ]
    }
   ],
   "source": [
    "app_1.propagate_labels(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Predicting Labels of Data Cells\n",
    "Raha then trains and applies one classifier per data column to predict the label of the rest of data cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A classifier is trained and applied on column 0.\n",
      "A classifier is trained and applied on column 1.\n",
      "A classifier is trained and applied on column 2.\n",
      "A classifier is trained and applied on column 3.\n",
      "A classifier is trained and applied on column 4.\n",
      "A classifier is trained and applied on column 5.\n",
      "A classifier is trained and applied on column 6.\n",
      "A classifier is trained and applied on column 7.\n",
      "A classifier is trained and applied on column 8.\n",
      "A classifier is trained and applied on column 9.\n",
      "A classifier is trained and applied on column 10.\n"
     ]
    }
   ],
   "source": [
    "app_1.predict_labels(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Storing Results\n",
    "Raha can also store the error detection results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results are stored in datasets/beers/raha-baran-results-beers/error-detection/detection.dataset.\n"
     ]
    }
   ],
   "source": [
    "app_1.store_results(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Evaluating the Error Detection Task\n",
    "We can finally evaluate our error detection task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raha's performance on beers:\n",
      "Precision = 1.00\n",
      "Recall = 1.00\n",
      "F1 = 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/source/MA/raha/raha/dataset.py:123: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if correction_dictionary[cell] == actual_errors[cell]:\n"
     ]
    }
   ],
   "source": [
    "p, r, f = d.get_data_cleaning_evaluation(d.detected_cells)[:3]\n",
    "print(\"Raha's performance on {}:\\nPrecision = {:.2f}\\nRecall = {:.2f}\\nF1 = {:.2f}\".format(d.name, p, r, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Correction with Baran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Instantiating the Correction Class\n",
    "We first instantiate the `Correction` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_2 = raha.Correction()\n",
    "\n",
    "# How many tuples would you label?\n",
    "app_2.LABELING_BUDGET = 20\n",
    "\n",
    "# Would you like to see the logs?\n",
    "app_2.VERBOSE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initializing the Dataset Object\n",
    "We next initialize the dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  index    id            beer_name                           style  \\\n0     1  1436             Pub Beer             American Pale Lager   \n1     2  2265          Devil's Cup         American Pale Ale (APA)   \n2     3  2264  Rise of the Phoenix                    American IPA   \n3     4  2263             Sinister  American Double / Imperial IPA   \n4     5  2262        Sex and Candy                    American IPA   \n\n       ounces    abv  ibu brewery_id               brewery_name  city state  \n0     12.0 oz   0.05  N/A        408  10 Barrel Brewing Company  Bend    OR  \n1    12.0 oz.  0.066  N/A        177        18th Street Brewery  Gary    IN  \n2  12.0 ounce  0.071  N/A        177        18th Street Brewery  Gary    IN  \n3     12.0 oz  0.09%  N/A        177        18th Street Brewery  Gary    IN  \n4    12.0 OZ.  0.075  N/A        177        18th Street Brewery  Gary    IN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>id</th>\n      <th>beer_name</th>\n      <th>style</th>\n      <th>ounces</th>\n      <th>abv</th>\n      <th>ibu</th>\n      <th>brewery_id</th>\n      <th>brewery_name</th>\n      <th>city</th>\n      <th>state</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1436</td>\n      <td>Pub Beer</td>\n      <td>American Pale Lager</td>\n      <td>12.0 oz</td>\n      <td>0.05</td>\n      <td>N/A</td>\n      <td>408</td>\n      <td>10 Barrel Brewing Company</td>\n      <td>Bend</td>\n      <td>OR</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>2265</td>\n      <td>Devil's Cup</td>\n      <td>American Pale Ale (APA)</td>\n      <td>12.0 oz.</td>\n      <td>0.066</td>\n      <td>N/A</td>\n      <td>177</td>\n      <td>18th Street Brewery</td>\n      <td>Gary</td>\n      <td>IN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>2264</td>\n      <td>Rise of the Phoenix</td>\n      <td>American IPA</td>\n      <td>12.0 ounce</td>\n      <td>0.071</td>\n      <td>N/A</td>\n      <td>177</td>\n      <td>18th Street Brewery</td>\n      <td>Gary</td>\n      <td>IN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>2263</td>\n      <td>Sinister</td>\n      <td>American Double / Imperial IPA</td>\n      <td>12.0 oz</td>\n      <td>0.09%</td>\n      <td>N/A</td>\n      <td>177</td>\n      <td>18th Street Brewery</td>\n      <td>Gary</td>\n      <td>IN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>2262</td>\n      <td>Sex and Candy</td>\n      <td>American IPA</td>\n      <td>12.0 OZ.</td>\n      <td>0.075</td>\n      <td>N/A</td>\n      <td>177</td>\n      <td>18th Street Brewery</td>\n      <td>Gary</td>\n      <td>IN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = app_2.initialize_dataset(d)\n",
    "d.dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Initializing the Error Corrector Models\n",
    "Baran initializes the error corrector models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error corrector models are initialized.\n"
     ]
    }
   ],
   "source": [
    "app_2.initialize_models(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Interactive Tuple Sampling, Labeling, Model updating, Feature Generation, and Correction Prediction\n",
    "Baran then iteratively samples a tuple. We should label data cells of each sampled tuple. It then udpates the models accordingly and generates a feature vector for each pair of a data error and a correction candidate. Finally, it trains and applies a classifier to each data column to predict the final correction of each data error. Since we already labeled tuples for Raha, we use the same labeled tuples and do not label new tuples here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error corrector models are updated with new labeled tuple 192.\n",
      "215102 pairs of (a data error, a potential correction) are featurized.\n",
      "Prediction Method in this step:\n",
      "    Column 4: Train\n",
      "    Column 5: Train\n",
      "    Column 6: No train 0\n",
      "    Column 9: Train\n",
      "    Column 10: Train\n",
      "Train sizes in this step:\n",
      "    Column 4: 21\n",
      "    Column 5: 207\n",
      "    Column 6: 963\n",
      "    Column 9: 1131\n",
      "    Column 10: 153\n",
      "Corrections identified in this step:\n",
      "    Column 4: 2125 Mean correction confidence: 0.6292802510655409\n",
      "    Column 5: 501 Mean correction confidence: 0.5273601408761199\n",
      "    Column 6: 0.0 Mean correction confidence: nan\n",
      "    Column 9: 119 Mean correction confidence: 0.9999999999999999\n",
      "    Column 10: 143 Mean correction confidence: 0.9233932273223042\n",
      "Corrections applied in this step:\n",
      "    Column 4: 2125 Real changes: 0\n",
      "    Column 5: 433 Real changes: 62\n",
      "    Column 6: 0 Real changes: 0\n",
      "    Column 9: 119 Real changes: 0\n",
      "    Column 10: 122 Real changes: 22\n",
      "63% (2753 / 4369) of data errors are corrected.\n",
      "The error corrector models are updated with new labeled tuple 323.\n",
      "216107 pairs of (a data error, a potential correction) are featurized.\n",
      "Prediction Method in this step:\n",
      "    Column 4: Train\n",
      "    Column 5: Train\n",
      "    Column 6: Train\n",
      "    Column 9: Train\n",
      "    Column 10: Train\n",
      "Train sizes in this step:\n",
      "    Column 4: 21\n",
      "    Column 5: 207\n",
      "    Column 6: 972\n",
      "    Column 9: 1131\n",
      "    Column 10: 153\n",
      "Corrections identified in this step:\n",
      "    Column 4: 2392 Mean correction confidence: 0.6188347672921399\n",
      "    Column 5: 498 Mean correction confidence: 0.527193385358584\n",
      "    Column 6: 996 Mean correction confidence: 0.9999999999999998\n",
      "    Column 9: 119 Mean correction confidence: 0.9999999999999999\n",
      "    Column 10: 143 Mean correction confidence: 0.9002059101452233\n",
      "Corrections applied in this step:\n",
      "    Column 4: 2390 Real changes: 0\n",
      "    Column 5: 431 Real changes: 113\n",
      "    Column 6: 996 Real changes: 0\n",
      "    Column 9: 119 Real changes: 2\n",
      "    Column 10: 121 Real changes: 43\n",
      "92% (4014 / 4369) of data errors are corrected.\n",
      "The error corrector models are updated with new labeled tuple 90.\n",
      "216298 pairs of (a data error, a potential correction) are featurized.\n",
      "Prediction Method in this step:\n",
      "    Column 4: Train\n",
      "    Column 5: Train\n",
      "    Column 6: Train\n",
      "    Column 9: Train\n",
      "    Column 10: Train\n",
      "Train sizes in this step:\n",
      "    Column 4: 21\n",
      "    Column 5: 207\n",
      "    Column 6: 972\n",
      "    Column 9: 1134\n",
      "    Column 10: 153\n",
      "Corrections identified in this step:\n",
      "    Column 4: 2390 Mean correction confidence: 0.578761411691808\n",
      "    Column 5: 501 Mean correction confidence: 0.5273601408761199\n",
      "    Column 6: 996 Mean correction confidence: 0.9999999999999998\n",
      "    Column 9: 119 Mean correction confidence: 0.9999999999999999\n",
      "    Column 10: 119 Mean correction confidence: 0.9999999999999999\n",
      "Corrections applied in this step:\n",
      "    Column 4: 2390 Real changes: 56\n",
      "    Column 5: 433 Real changes: 116\n",
      "    Column 6: 996 Real changes: 0\n",
      "    Column 9: 119 Real changes: 2\n",
      "    Column 10: 119 Real changes: 33\n",
      "92% (4036 / 4369) of data errors are corrected.\n",
      "The error corrector models are updated with new labeled tuple 1121.\n",
      "216513 pairs of (a data error, a potential correction) are featurized.\n",
      "Prediction Method in this step:\n",
      "    Column 4: Train\n",
      "    Column 5: Train\n",
      "    Column 6: Train\n",
      "    Column 9: Train\n",
      "    Column 10: Train\n",
      "Train sizes in this step:\n",
      "    Column 4: 22\n",
      "    Column 5: 207\n",
      "    Column 6: 972\n",
      "    Column 9: 1134\n",
      "    Column 10: 153\n",
      "Corrections identified in this step:\n",
      "    Column 4: 1980 Mean correction confidence: 0.9835925551862856\n",
      "    Column 5: 694 Mean correction confidence: 0.9941662254713526\n",
      "    Column 6: 996 Mean correction confidence: 0.9999999999999998\n",
      "    Column 9: 119 Mean correction confidence: 0.9999999999999999\n",
      "    Column 10: 119 Mean correction confidence: 0.9999999999999999\n",
      "Corrections applied in this step:\n",
      "    Column 4: 1833 Real changes: 208\n",
      "    Column 5: 577 Real changes: 319\n",
      "    Column 6: 996 Real changes: 0\n",
      "    Column 9: 119 Real changes: 0\n",
      "    Column 10: 119 Real changes: 0\n",
      "97% (4243 / 4369) of data errors are corrected.\n",
      "The error corrector models are updated with new labeled tuple 1185.\n",
      "216513 pairs of (a data error, a potential correction) are featurized.\n",
      "Prediction Method in this step:\n",
      "    Column 4: Train\n",
      "    Column 5: Train\n",
      "    Column 6: Train\n",
      "    Column 9: Train\n",
      "    Column 10: Train\n",
      "Train sizes in this step:\n",
      "    Column 4: 22\n",
      "    Column 5: 207\n",
      "    Column 6: 972\n",
      "    Column 9: 1134\n",
      "    Column 10: 153\n",
      "Corrections identified in this step:\n",
      "    Column 4: 1983 Mean correction confidence: 0.9833611308235113\n",
      "    Column 5: 548 Mean correction confidence: 0.9999999999999998\n",
      "    Column 6: 996 Mean correction confidence: 0.9999999999999998\n",
      "    Column 9: 119 Mean correction confidence: 0.9999999999999999\n",
      "    Column 10: 119 Mean correction confidence: 0.9999999999999999\n",
      "Corrections applied in this step:\n",
      "    Column 4: 1836 Real changes: 0\n",
      "    Column 5: 548 Real changes: 190\n",
      "    Column 6: 996 Real changes: 0\n",
      "    Column 9: 119 Real changes: 0\n",
      "    Column 10: 119 Real changes: 0\n",
      "98% (4298 / 4369) of data errors are corrected.\n",
      "The error corrector models are updated with new labeled tuple 1666.\n",
      "216513 pairs of (a data error, a potential correction) are featurized.\n",
      "Prediction Method in this step:\n",
      "    Column 4: Train\n",
      "    Column 5: Train\n",
      "    Column 6: Train\n",
      "    Column 9: Train\n",
      "    Column 10: Train\n",
      "Train sizes in this step:\n",
      "    Column 4: 22\n",
      "    Column 5: 207\n",
      "    Column 6: 972\n",
      "    Column 9: 1134\n",
      "    Column 10: 153\n",
      "Corrections identified in this step:\n",
      "    Column 4: 1974 Mean correction confidence: 0.9831217774340839\n",
      "    Column 5: 548 Mean correction confidence: 0.9999999999999998\n",
      "    Column 6: 996 Mean correction confidence: 0.9999999999999998\n",
      "    Column 9: 119 Mean correction confidence: 0.9999999999999999\n",
      "    Column 10: 119 Mean correction confidence: 0.9999999999999999\n",
      "Corrections applied in this step:\n",
      "    Column 4: 1832 Real changes: 0\n",
      "    Column 5: 548 Real changes: 0\n",
      "    Column 6: 996 Real changes: 0\n",
      "    Column 9: 119 Real changes: 2\n",
      "    Column 10: 119 Real changes: 0\n",
      "98% (4298 / 4369) of data errors are corrected.\n",
      "The error corrector models are updated with new labeled tuple 2251.\n",
      "216513 pairs of (a data error, a potential correction) are featurized.\n",
      "Prediction Method in this step:\n",
      "    Column 4: Train\n",
      "    Column 5: Train\n",
      "    Column 6: Train\n",
      "    Column 9: Train\n",
      "    Column 10: Train\n",
      "Train sizes in this step:\n",
      "    Column 4: 22\n",
      "    Column 5: 207\n",
      "    Column 6: 972\n",
      "    Column 9: 1134\n",
      "    Column 10: 153\n",
      "Corrections identified in this step:\n",
      "    Column 4: 1864 Mean correction confidence: 0.9640737514535437\n",
      "    Column 5: 548 Mean correction confidence: 0.9999999999999998\n",
      "    Column 6: 996 Mean correction confidence: 0.9999999999999998\n",
      "    Column 9: 119 Mean correction confidence: 0.9999999999999999\n",
      "    Column 10: 119 Mean correction confidence: 0.9999999999999999\n",
      "Corrections applied in this step:\n",
      "    Column 4: 1714 Real changes: 0\n",
      "    Column 5: 548 Real changes: 0\n",
      "    Column 6: 996 Real changes: 0\n",
      "    Column 9: 119 Real changes: 2\n",
      "    Column 10: 119 Real changes: 0\n",
      "98% (4298 / 4369) of data errors are corrected.\n",
      "The error corrector models are updated with new labeled tuple 2395.\n",
      "218351 pairs of (a data error, a potential correction) are featurized.\n",
      "Prediction Method in this step:\n",
      "    Column 4: Train\n",
      "    Column 5: Train\n",
      "    Column 6: Train\n",
      "    Column 9: Train\n",
      "    Column 10: Train\n",
      "Train sizes in this step:\n",
      "    Column 4: 40\n",
      "    Column 5: 207\n",
      "    Column 6: 972\n",
      "    Column 9: 1134\n",
      "    Column 10: 153\n",
      "Corrections identified in this step:\n",
      "    Column 4: 2484 Mean correction confidence: 0.9385573661932903\n",
      "    Column 5: 548 Mean correction confidence: 0.9999999999999998\n",
      "    Column 6: 996 Mean correction confidence: 0.9999999999999998\n",
      "    Column 9: 119 Mean correction confidence: 0.9999999999999999\n",
      "    Column 10: 119 Mean correction confidence: 0.9999999999999999\n",
      "Corrections applied in this step:\n",
      "    Column 4: 2286 Real changes: 222\n",
      "    Column 5: 548 Real changes: 0\n",
      "    Column 6: 996 Real changes: 0\n",
      "    Column 9: 119 Real changes: 0\n",
      "    Column 10: 119 Real changes: 0\n",
      "98% (4298 / 4369) of data errors are corrected.\n",
      "The error corrector models are updated with new labeled tuple 1039.\n",
      "218351 pairs of (a data error, a potential correction) are featurized.\n",
      "Prediction Method in this step:\n",
      "    Column 4: Train\n",
      "    Column 5: Train\n",
      "    Column 6: Train\n",
      "    Column 9: Train\n",
      "    Column 10: Train\n",
      "Train sizes in this step:\n",
      "    Column 4: 40\n",
      "    Column 5: 207\n",
      "    Column 6: 972\n",
      "    Column 9: 1134\n",
      "    Column 10: 153\n",
      "Corrections identified in this step:\n",
      "    Column 4: 2447 Mean correction confidence: 0.9517937977869114\n",
      "    Column 5: 548 Mean correction confidence: 0.9999999999999998\n",
      "    Column 6: 996 Mean correction confidence: 0.9999999999999998\n",
      "    Column 9: 119 Mean correction confidence: 0.9999999999999999\n",
      "    Column 10: 119 Mean correction confidence: 0.9999999999999999\n",
      "Corrections applied in this step:\n",
      "    Column 4: 2253 Real changes: 4\n",
      "    Column 5: 548 Real changes: 0\n",
      "    Column 6: 996 Real changes: 0\n",
      "    Column 9: 119 Real changes: 2\n",
      "    Column 10: 119 Real changes: 0\n",
      "98% (4298 / 4369) of data errors are corrected.\n",
      "The error corrector models are updated with new labeled tuple 442.\n",
      "218351 pairs of (a data error, a potential correction) are featurized.\n",
      "Prediction Method in this step:\n",
      "    Column 4: Train\n",
      "    Column 5: Train\n",
      "    Column 6: Train\n",
      "    Column 9: Train\n",
      "    Column 10: Train\n",
      "Train sizes in this step:\n",
      "    Column 4: 40\n",
      "    Column 5: 207\n",
      "    Column 6: 972\n",
      "    Column 9: 1134\n",
      "    Column 10: 153\n",
      "Corrections identified in this step:\n",
      "    Column 4: 2432 Mean correction confidence: 0.8844220757472366\n",
      "    Column 5: 548 Mean correction confidence: 0.9999999999999998\n",
      "    Column 6: 996 Mean correction confidence: 0.9999999999999998\n",
      "    Column 9: 119 Mean correction confidence: 0.9999999999999999\n",
      "    Column 10: 119 Mean correction confidence: 0.9999999999999999\n",
      "Corrections applied in this step:\n",
      "    Column 4: 2203 Real changes: 106\n",
      "    Column 5: 548 Real changes: 0\n",
      "    Column 6: 996 Real changes: 0\n",
      "    Column 9: 119 Real changes: 2\n",
      "    Column 10: 119 Real changes: 0\n",
      "98% (4298 / 4369) of data errors are corrected.\n",
      "The error corrector models are updated with new labeled tuple 1955.\n",
      "218352 pairs of (a data error, a potential correction) are featurized.\n",
      "Prediction Method in this step:\n",
      "    Column 4: Train\n",
      "    Column 5: Train\n",
      "    Column 6: Train\n",
      "    Column 9: Train\n",
      "    Column 10: Train\n",
      "Train sizes in this step:\n",
      "    Column 4: 40\n",
      "    Column 5: 207\n",
      "    Column 6: 972\n",
      "    Column 9: 1134\n",
      "    Column 10: 153\n",
      "Corrections identified in this step:\n",
      "    Column 4: 2066 Mean correction confidence: 0.9218599969248548\n",
      "    Column 5: 548 Mean correction confidence: 0.9999999999999998\n",
      "    Column 6: 996 Mean correction confidence: 0.9999999999999998\n",
      "    Column 9: 36 Mean correction confidence: 0.9999999999999998\n",
      "    Column 10: 0 Mean correction confidence: nan\n",
      "Corrections applied in this step:\n",
      "    Column 4: 1997 Real changes: 87\n",
      "    Column 5: 548 Real changes: 0\n",
      "    Column 6: 996 Real changes: 0\n",
      "    Column 9: 36 Real changes: 33\n",
      "    Column 10: 0 Real changes: 0\n",
      "98% (4301 / 4369) of data errors are corrected.\n",
      "The error corrector models are updated with new labeled tuple 388.\n",
      "218352 pairs of (a data error, a potential correction) are featurized.\n",
      "Prediction Method in this step:\n",
      "    Column 4: Train\n",
      "    Column 5: Train\n",
      "    Column 6: Train\n",
      "    Column 9: Train\n",
      "    Column 10: Train\n",
      "Train sizes in this step:\n",
      "    Column 4: 40\n",
      "    Column 5: 207\n",
      "    Column 6: 972\n",
      "    Column 9: 1134\n",
      "    Column 10: 153\n",
      "Corrections identified in this step:\n",
      "    Column 4: 2075 Mean correction confidence: 0.9095438809811152\n",
      "    Column 5: 548 Mean correction confidence: 0.9999999999999998\n",
      "    Column 6: 996 Mean correction confidence: 0.9999999999999998\n",
      "    Column 9: 0 Mean correction confidence: nan\n",
      "    Column 10: 0 Mean correction confidence: nan\n",
      "Corrections applied in this step:\n",
      "    Column 4: 2024 Real changes: 65\n",
      "    Column 5: 548 Real changes: 0\n",
      "    Column 6: 996 Real changes: 0\n",
      "    Column 9: 0 Real changes: 0\n",
      "    Column 10: 0 Real changes: 0\n",
      "98% (4301 / 4369) of data errors are corrected.\n",
      "The error corrector models are updated with new labeled tuple 1969.\n",
      "218352 pairs of (a data error, a potential correction) are featurized.\n",
      "Prediction Method in this step:\n",
      "    Column 4: Train\n",
      "    Column 5: Train\n",
      "    Column 6: Train\n",
      "    Column 9: Train\n",
      "    Column 10: Train\n",
      "Train sizes in this step:\n",
      "    Column 4: 40\n",
      "    Column 5: 207\n",
      "    Column 6: 972\n",
      "    Column 9: 1134\n",
      "    Column 10: 153\n",
      "Corrections identified in this step:\n",
      "    Column 4: 2068 Mean correction confidence: 0.9242404985036485\n",
      "    Column 5: 548 Mean correction confidence: 0.9999999999999998\n",
      "    Column 6: 996 Mean correction confidence: 0.9999999999999998\n",
      "    Column 9: 119 Mean correction confidence: 0.9999999999999999\n",
      "    Column 10: 119 Mean correction confidence: 0.9999999999999999\n",
      "Corrections applied in this step:\n",
      "    Column 4: 1997 Real changes: 63\n",
      "    Column 5: 548 Real changes: 0\n",
      "    Column 6: 996 Real changes: 0\n",
      "    Column 9: 119 Real changes: 35\n",
      "    Column 10: 119 Real changes: 0\n",
      "98% (4301 / 4369) of data errors are corrected.\n",
      "The error corrector models are updated with new labeled tuple 2394.\n",
      "218352 pairs of (a data error, a potential correction) are featurized.\n",
      "Prediction Method in this step:\n",
      "    Column 4: Train\n",
      "    Column 5: Train\n",
      "    Column 6: Train\n",
      "    Column 9: Train\n",
      "    Column 10: Train\n",
      "Train sizes in this step:\n",
      "    Column 4: 40\n",
      "    Column 5: 207\n",
      "    Column 6: 972\n",
      "    Column 9: 1134\n",
      "    Column 10: 153\n",
      "Corrections identified in this step:\n",
      "    Column 4: 2087 Mean correction confidence: 0.9108313233109971\n",
      "    Column 5: 548 Mean correction confidence: 0.9999999999999998\n",
      "    Column 6: 996 Mean correction confidence: 0.9999999999999998\n",
      "    Column 9: 14 Mean correction confidence: 0.9999999999999999\n",
      "    Column 10: 119 Mean correction confidence: 0.9999999999999999\n",
      "Corrections applied in this step:\n",
      "    Column 4: 2030 Real changes: 65\n",
      "    Column 5: 548 Real changes: 0\n",
      "    Column 6: 996 Real changes: 0\n",
      "    Column 9: 14 Real changes: 1\n",
      "    Column 10: 119 Real changes: 0\n",
      "98% (4301 / 4369) of data errors are corrected.\n",
      "The error corrector models are updated with new labeled tuple 2391.\n",
      "218352 pairs of (a data error, a potential correction) are featurized.\n",
      "Prediction Method in this step:\n",
      "    Column 4: Train\n",
      "    Column 5: Train\n",
      "    Column 6: Train\n",
      "    Column 9: Train\n",
      "    Column 10: Train\n",
      "Train sizes in this step:\n",
      "    Column 4: 40\n",
      "    Column 5: 207\n",
      "    Column 6: 972\n",
      "    Column 9: 1134\n",
      "    Column 10: 153\n",
      "Corrections identified in this step:\n",
      "    Column 4: 2055 Mean correction confidence: 0.9379309220452539\n",
      "    Column 5: 548 Mean correction confidence: 0.9999999999999998\n",
      "    Column 6: 996 Mean correction confidence: 0.9999999999999998\n",
      "    Column 9: 0 Mean correction confidence: nan\n",
      "    Column 10: 119 Mean correction confidence: 0.9999999999999999\n",
      "Corrections applied in this step:\n",
      "    Column 4: 2006 Real changes: 79\n",
      "    Column 5: 548 Real changes: 0\n",
      "    Column 6: 996 Real changes: 0\n",
      "    Column 9: 0 Real changes: 0\n",
      "    Column 10: 119 Real changes: 0\n",
      "98% (4301 / 4369) of data errors are corrected.\n",
      "The error corrector models are updated with new labeled tuple 1579.\n",
      "218352 pairs of (a data error, a potential correction) are featurized.\n",
      "Prediction Method in this step:\n",
      "    Column 4: Train\n",
      "    Column 5: Train\n",
      "    Column 6: Train\n",
      "    Column 9: Train\n",
      "    Column 10: Train\n",
      "Train sizes in this step:\n",
      "    Column 4: 40\n",
      "    Column 5: 207\n",
      "    Column 6: 972\n",
      "    Column 9: 1134\n",
      "    Column 10: 153\n",
      "Corrections identified in this step:\n",
      "    Column 4: 2198 Mean correction confidence: 0.9410378482578652\n",
      "    Column 5: 25 Mean correction confidence: 0.9999999999999997\n",
      "    Column 6: 996 Mean correction confidence: 0.9999999999999998\n",
      "    Column 9: 0 Mean correction confidence: nan\n",
      "    Column 10: 119 Mean correction confidence: 0.9999999999999999\n",
      "Corrections applied in this step:\n",
      "    Column 4: 2069 Real changes: 65\n",
      "    Column 5: 25 Real changes: 0\n",
      "    Column 6: 996 Real changes: 0\n",
      "    Column 9: 0 Real changes: 0\n",
      "    Column 10: 119 Real changes: 0\n",
      "98% (4301 / 4369) of data errors are corrected.\n",
      "The error corrector models are updated with new labeled tuple 2260.\n",
      "218352 pairs of (a data error, a potential correction) are featurized.\n",
      "Prediction Method in this step:\n",
      "    Column 4: Train\n",
      "    Column 5: Train\n",
      "    Column 6: Train\n",
      "    Column 9: Train\n",
      "    Column 10: Train\n",
      "Train sizes in this step:\n",
      "    Column 4: 40\n",
      "    Column 5: 207\n",
      "    Column 6: 972\n",
      "    Column 9: 1134\n",
      "    Column 10: 153\n",
      "Corrections identified in this step:\n",
      "    Column 4: 2075 Mean correction confidence: 0.9999948695206318\n",
      "    Column 5: 51 Mean correction confidence: 0.9999999999999999\n",
      "    Column 6: 996 Mean correction confidence: 0.9999999999999998\n",
      "    Column 9: 0 Mean correction confidence: nan\n",
      "    Column 10: 0 Mean correction confidence: nan\n",
      "Corrections applied in this step:\n",
      "    Column 4: 2075 Real changes: 175\n",
      "    Column 5: 51 Real changes: 12\n",
      "    Column 6: 996 Real changes: 0\n",
      "    Column 9: 0 Real changes: 0\n",
      "    Column 10: 0 Real changes: 0\n",
      "99% (4304 / 4369) of data errors are corrected.\n",
      "The error corrector models are updated with new labeled tuple 553.\n",
      "218352 pairs of (a data error, a potential correction) are featurized.\n",
      "Prediction Method in this step:\n",
      "    Column 4: Train\n",
      "    Column 5: Train\n",
      "    Column 6: Train\n",
      "    Column 9: Train\n",
      "    Column 10: Train\n",
      "Train sizes in this step:\n",
      "    Column 4: 40\n",
      "    Column 5: 207\n",
      "    Column 6: 972\n",
      "    Column 9: 1134\n",
      "    Column 10: 153\n",
      "Corrections identified in this step:\n",
      "    Column 4: 789 Mean correction confidence: 0.9979759153652372\n",
      "    Column 5: 0 Mean correction confidence: nan\n",
      "    Column 6: 0 Mean correction confidence: nan\n",
      "    Column 9: 36 Mean correction confidence: 0.9999999999999998\n",
      "    Column 10: 0 Mean correction confidence: nan\n",
      "Corrections applied in this step:\n",
      "    Column 4: 789 Real changes: 65\n",
      "    Column 5: 0 Real changes: 0\n",
      "    Column 6: 0 Real changes: 0\n",
      "    Column 9: 36 Real changes: 33\n",
      "    Column 10: 0 Real changes: 0\n",
      "99% (4304 / 4369) of data errors are corrected.\n",
      "The error corrector models are updated with new labeled tuple 2221.\n",
      "218356 pairs of (a data error, a potential correction) are featurized.\n",
      "Prediction Method in this step:\n",
      "    Column 4: Train\n",
      "    Column 5: Train\n",
      "    Column 6: Train\n",
      "    Column 9: Train\n",
      "    Column 10: Train\n",
      "Train sizes in this step:\n",
      "    Column 4: 40\n",
      "    Column 5: 207\n",
      "    Column 6: 972\n",
      "    Column 9: 1134\n",
      "    Column 10: 153\n",
      "Corrections identified in this step:\n",
      "    Column 4: 1854 Mean correction confidence: 0.9999999999999999\n",
      "    Column 5: 0 Mean correction confidence: nan\n",
      "    Column 6: 0 Mean correction confidence: nan\n",
      "    Column 9: 14 Mean correction confidence: 0.9999999999999999\n",
      "    Column 10: 14 Mean correction confidence: 0.9999999999999999\n",
      "Corrections applied in this step:\n",
      "    Column 4: 1854 Real changes: 200\n",
      "    Column 5: 0 Real changes: 0\n",
      "    Column 6: 0 Real changes: 0\n",
      "    Column 9: 14 Real changes: 4\n",
      "    Column 10: 14 Real changes: 1\n",
      "99% (4304 / 4369) of data errors are corrected.\n",
      "The error corrector models are updated with new labeled tuple 1000.\n",
      "218356 pairs of (a data error, a potential correction) are featurized.\n",
      "Prediction Method in this step:\n",
      "    Column 4: Train\n",
      "    Column 5: Train\n",
      "    Column 6: Train\n",
      "    Column 9: Train\n",
      "    Column 10: Train\n",
      "Train sizes in this step:\n",
      "    Column 4: 40\n",
      "    Column 5: 207\n",
      "    Column 6: 972\n",
      "    Column 9: 1134\n",
      "    Column 10: 153\n",
      "Corrections identified in this step:\n",
      "    Column 4: 0 Mean correction confidence: nan\n",
      "    Column 5: 0 Mean correction confidence: nan\n",
      "    Column 6: 0 Mean correction confidence: nan\n",
      "    Column 9: 0 Mean correction confidence: nan\n",
      "    Column 10: 14 Mean correction confidence: 0.9999999999999999\n",
      "Corrections applied in this step:\n",
      "    Column 4: 0 Real changes: 0\n",
      "    Column 5: 0 Real changes: 0\n",
      "    Column 6: 0 Real changes: 0\n",
      "    Column 9: 0 Real changes: 0\n",
      "    Column 10: 14 Real changes: 0\n",
      "99% (4304 / 4369) of data errors are corrected.\n"
     ]
    }
   ],
   "source": [
    "# while len(d.labeled_tuples) < app_2.LABELING_BUDGET:\n",
    "#     app_2.sample_tuple(d)\n",
    "#     if d.has_ground_truth:\n",
    "#         app_2.label_with_ground_truth(d)\n",
    "#     else:\n",
    "#         print(\"Label the dirty cells in the following sampled tuple.\")\n",
    "#         sampled_tuple = pandas.DataFrame(data=[d.dataframe.iloc[d.sampled_tuple, :]], columns=d.dataframe.columns)\n",
    "#         IPython.display.display(sampled_tuple)\n",
    "#         for j in range(d.dataframe.shape[1]):\n",
    "#             cell = (d.sampled_tuple, j)\n",
    "#             value = d.dataframe.iloc[cell]\n",
    "#             correction = input(\"What is the correction for value '{}'? Type in the same value if it is not erronous.\\n\".format(value))\n",
    "#             user_label = 1 if value != correction else 0\n",
    "#             d.labeled_cells[cell] = [user_label, correction]\n",
    "#         d.labeled_tuples[d.sampled_tuple] = 1\n",
    "#     app_2.update_models(d)\n",
    "#     app_2.generate_features(d)\n",
    "#     app_2.predict_corrections(d)\n",
    "\n",
    "#labeled_tuples_list = list(d.labeled_tuples.keys())\n",
    "#k = labeled_tuples_list[0]\n",
    "\n",
    "#si = d.labeled_tuples[k]\n",
    "#d.sampled_tuple = k\n",
    "#app_2.update_models(d)\n",
    "#app_2.generate_features(d)\n",
    "\n",
    "for si in d.labeled_tuples:\n",
    "    d.sampled_tuple = si\n",
    "    app_2.update_models(d)\n",
    "    app_2.generate_features(d)\n",
    "    app_2.predict_corrections(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "#d.corrected_cells = {}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "#app_2.predict_corrections(d)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Storing Results\n",
    "Baran can also store the error correction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results are stored in datasets/beers/raha-baran-results-beers/error-correction/correction.dataset.\n"
     ]
    }
   ],
   "source": [
    "app_2.store_results(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluating the Error Correction Task\n",
    "We can finally evaluate our error correction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baran's performance on beers:\n",
      "Precision = 0.94\n",
      "Recall = 0.93\n",
      "F1 = 0.93\n"
     ]
    }
   ],
   "source": [
    "p, r, f = d.get_data_cleaning_evaluation(d.corrected_cells)[-3:]\n",
    "print(\"Baran's performance on {}:\\nPrecision = {:.2f}\\nRecall = {:.2f}\\nF1 = {:.2f}\".format(d.name, p, r, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Further detection evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "actual_errors = d.get_actual_errors_dictionary()\n",
    "len(actual_errors)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "#d = pickle.load(open(\"datasets/flights/raha-baran-results-flights/error-correction/correction.dataset\", \"rb\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "26510"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.dataframe.shape[0] * d.dataframe.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "import importlib\n",
    "from raha import analysis_utilities"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histograms of the probabilities of the detection algorithm by (label, true label)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 4 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "importlib.reload(analysis_utilities)\n",
    "\n",
    "analysis_utilities.detection_evaluation(d, actual_errors)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Correction feature analysis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "correction_features = []\n",
    "\n",
    "for cell in d.corrected_cells:\n",
    "    correction_features.append(list(cell) +\n",
    "                               [d.dataframe.iloc[cell]] +\n",
    "                               [d.corrected_cells[cell]] +\n",
    "                               [actual_errors[cell] if cell in actual_errors else d.dataframe.iloc[cell]] +\n",
    "                               list(d.pair_features[cell][d.corrected_cells[cell]]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "correction_features_df = DataFrame(correction_features,\n",
    "                                   columns=[\"row\",\n",
    "                                            \"column\",\n",
    "                                            \"old_value\",\n",
    "                                            \"new_value\",\n",
    "                                            \"actual_value\",\n",
    "                                            *[f\"feature_value_{i}\" for i in range(8)],\n",
    "                                            *[f\"feature_vicinity_{i}\" for i in range(d.dataframe.shape[1])],\n",
    "                                            \"feature_domain\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "feature_value_4        0.000000\nfeature_value_5        0.000000\nfeature_vicinity_4     0.000000\nfeature_value_2        0.002169\nfeature_value_3        0.002169\nfeature_vicinity_1     0.008829\nfeature_vicinity_0     0.008829\nfeature_vicinity_2     0.027138\nfeature_vicinity_6     0.051442\nfeature_vicinity_9     0.059025\nfeature_vicinity_7     0.111781\nfeature_vicinity_8     0.114336\nfeature_vicinity_5     0.131783\nfeature_vicinity_3     0.185038\nfeature_vicinity_10    0.185315\nfeature_domain         0.323399\nfeature_value_7        0.570616\nfeature_value_0        0.676115\nfeature_value_6        0.678284\nfeature_value_1        0.890567\ndtype: float64"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction_features_df.iloc[:,5:].mean().sort_values()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "        feature_value_0  feature_value_1  feature_value_2  feature_value_3  \\\ncolumn                                                                       \n4              0.777593         0.945228         0.000000         0.000000   \n5              0.043818         0.851330         0.000000         0.000000   \n6              1.000000         1.000000         0.000000         0.000000   \n9              0.024000         0.048000         0.000000         0.000000   \n10             0.000000         0.000000         0.074667         0.074667   \n\n        feature_value_4  feature_value_5  feature_value_6  feature_value_7  \\\ncolumn                                                                       \n4                   0.0              0.0         0.777593         0.575768   \n5                   0.0              0.0         0.043818         0.029734   \n6                   0.0              0.0         1.000000         1.000000   \n9                   0.0              0.0         0.024000         0.280000   \n10                  0.0              0.0         0.074667         0.074667   \n\n        feature_vicinity_0  feature_vicinity_1  feature_vicinity_2  \\\ncolumn                                                               \n4                 0.008299            0.008299            0.008714   \n5                 0.004695            0.004695            0.083881   \n6                 0.008955            0.008955            0.008955   \n9                 0.024000            0.024000            0.132800   \n10                0.024000            0.024000            0.132800   \n\n        feature_vicinity_3  feature_vicinity_4  feature_vicinity_5  \\\ncolumn                                                               \n4                 0.288313                 0.0            0.203320   \n5                 0.074989                 0.0            0.000000   \n6                 0.042727                 0.0            0.067173   \n9                 0.021662                 0.0            0.022872   \n10                0.064030                 0.0            0.054594   \n\n        feature_vicinity_6  feature_vicinity_7  feature_vicinity_8  \\\ncolumn                                                               \n4                 0.074689            0.054772            0.060166   \n5                 0.051269            0.098513            0.099977   \n6                 0.000000            0.072790            0.072790   \n9                 0.019034            0.736000            0.712520   \n10                0.050130            0.968000            0.968000   \n\n        feature_vicinity_9  feature_vicinity_10  feature_domain  \ncolumn                                                           \n4                 0.071784             0.312725        0.562490  \n5                 0.071422             0.049827        0.036176  \n6                 0.035227             0.012028        0.006365  \n9                 0.000000             0.000000        0.006596  \n10                0.000000             0.000000        0.047773  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature_value_0</th>\n      <th>feature_value_1</th>\n      <th>feature_value_2</th>\n      <th>feature_value_3</th>\n      <th>feature_value_4</th>\n      <th>feature_value_5</th>\n      <th>feature_value_6</th>\n      <th>feature_value_7</th>\n      <th>feature_vicinity_0</th>\n      <th>feature_vicinity_1</th>\n      <th>feature_vicinity_2</th>\n      <th>feature_vicinity_3</th>\n      <th>feature_vicinity_4</th>\n      <th>feature_vicinity_5</th>\n      <th>feature_vicinity_6</th>\n      <th>feature_vicinity_7</th>\n      <th>feature_vicinity_8</th>\n      <th>feature_vicinity_9</th>\n      <th>feature_vicinity_10</th>\n      <th>feature_domain</th>\n    </tr>\n    <tr>\n      <th>column</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4</th>\n      <td>0.777593</td>\n      <td>0.945228</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.777593</td>\n      <td>0.575768</td>\n      <td>0.008299</td>\n      <td>0.008299</td>\n      <td>0.008714</td>\n      <td>0.288313</td>\n      <td>0.0</td>\n      <td>0.203320</td>\n      <td>0.074689</td>\n      <td>0.054772</td>\n      <td>0.060166</td>\n      <td>0.071784</td>\n      <td>0.312725</td>\n      <td>0.562490</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.043818</td>\n      <td>0.851330</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.043818</td>\n      <td>0.029734</td>\n      <td>0.004695</td>\n      <td>0.004695</td>\n      <td>0.083881</td>\n      <td>0.074989</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.051269</td>\n      <td>0.098513</td>\n      <td>0.099977</td>\n      <td>0.071422</td>\n      <td>0.049827</td>\n      <td>0.036176</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.008955</td>\n      <td>0.008955</td>\n      <td>0.008955</td>\n      <td>0.042727</td>\n      <td>0.0</td>\n      <td>0.067173</td>\n      <td>0.000000</td>\n      <td>0.072790</td>\n      <td>0.072790</td>\n      <td>0.035227</td>\n      <td>0.012028</td>\n      <td>0.006365</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.024000</td>\n      <td>0.048000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.024000</td>\n      <td>0.280000</td>\n      <td>0.024000</td>\n      <td>0.024000</td>\n      <td>0.132800</td>\n      <td>0.021662</td>\n      <td>0.0</td>\n      <td>0.022872</td>\n      <td>0.019034</td>\n      <td>0.736000</td>\n      <td>0.712520</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.006596</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.074667</td>\n      <td>0.074667</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.074667</td>\n      <td>0.074667</td>\n      <td>0.024000</td>\n      <td>0.024000</td>\n      <td>0.132800</td>\n      <td>0.064030</td>\n      <td>0.0</td>\n      <td>0.054594</td>\n      <td>0.050130</td>\n      <td>0.968000</td>\n      <td>0.968000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.047773</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction_features_df.iloc[:,[1] + list(range(5,correction_features_df.shape[1]))].groupby(\"column\").mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "correction_features_df[\"wrong_correction\"] = (correction_features_df[\"old_value\"] == correction_features_df[\"actual_value\"]) & (correction_features_df[\"old_value\"] != correction_features_df[\"new_value\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "0.0"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction_features_df[\"wrong_correction\"].sum() / correction_features_df.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "[1162, 1171, 2311, 282, 160, 358, 237, 693, 745, 619, 149]"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_features = []\n",
    "\n",
    "for column_index in range(d.dataframe.shape[1]):\n",
    "    unique_rows = np.unique(d.column_features[column_index], axis=0)\n",
    "    distinct_features.append(unique_rows.shape[0])\n",
    "\n",
    "distinct_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "cluster_features = []\n",
    "\n",
    "for cluster_i in range(1,21):\n",
    "    cluster_features.append(d.column_features[3][\n",
    "        [key[0] for key in d.clusters_k_j_c_ce[21][3][cluster_i].keys()]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [1.        , 0.        , 1.        , ..., 0.        , 0.        ,\n        1.        ],\n       [0.99363057, 0.8089172 , 0.23566879, ..., 0.        , 0.        ,\n        0.98726115],\n       ...,\n       [1.        , 0.        , 1.        , ..., 0.        , 0.        ,\n        0.        ],\n       [1.        , 0.        , 1.        , ..., 0.        , 0.        ,\n        0.        ],\n       [1.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ]])"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_feature_means = np.array([cluster.mean(axis=0) for cluster in cluster_features])\n",
    "cluster_feature_means"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "data": {
      "text/plain": "1       6\n2       4\n3       1\n5       1\n7       1\n26      1\n30      1\n53      1\n59      1\n69      1\n86      1\n157     1\n1901    1\ndtype: int64"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Series([len(cluster) for cluster in d.clusters_k_j_c_ce[21][3].values()]).value_counts().sort_index()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Correction prediction analysis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = d.correction_prediction_dfs[0][5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "501"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"prediction\"].sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "cell\n(3, 5)       0\n(1403, 5)    0\n(1402, 5)    0\n(1397, 5)    0\n(1368, 5)    0\n(1364, 5)    0\n(1359, 5)    0\n(1334, 5)    0\n(1333, 5)    0\n(1321, 5)    0\n(1307, 5)    0\n(1306, 5)    0\n(1421, 5)    0\n(1295, 5)    0\n(1288, 5)    0\n(1283, 5)    0\n(1273, 5)    0\n(1238, 5)    0\n(1235, 5)    0\n(1232, 5)    0\n(1213, 5)    0\n(1203, 5)    0\n(1182, 5)    0\n(1181, 5)    0\n(1177, 5)    0\n(1293, 5)    0\n(1157, 5)    0\n(1423, 5)    0\n(1430, 5)    0\n(1634, 5)    0\n            ..\n(1988, 5)    2\n(1165, 5)    2\n(179, 5)     2\n(2124, 5)    2\n(1162, 5)    2\n(1951, 5)    3\n(1614, 5)    3\n(1603, 5)    3\n(78, 5)      3\n(2141, 5)    3\n(2197, 5)    3\n(1131, 5)    3\n(1933, 5)    3\n(1223, 5)    3\n(1254, 5)    3\n(501, 5)     3\n(788, 5)     3\n(1695, 5)    4\n(1952, 5)    4\n(1474, 5)    4\n(1928, 5)    4\n(1925, 5)    4\n(1226, 5)    4\n(669, 5)     4\n(681, 5)     4\n(1947, 5)    4\n(1233, 5)    4\n(667, 5)     5\n(685, 5)     5\n(331, 5)     6\nName: prediction, Length: 690, dtype: int64"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"prediction\"].groupby(df[\"cell\"]).sum().sort_values()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "#df = df[df[\"prediction\"] == 1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.axes._subplots.AxesSubplot at 0x7f52ccdd4290>"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"probability\"].hist(bins=np.linspace(0.0, 1.0, 21))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "           prediction       cell  \\\ncell                               \n(12, 5)             1    (12, 5)   \n(26, 5)             1    (26, 5)   \n(35, 5)             1    (35, 5)   \n(44, 5)             2    (44, 5)   \n(46, 5)             2    (46, 5)   \n(70, 5)             1    (70, 5)   \n(78, 5)             3    (78, 5)   \n(79, 5)             1    (79, 5)   \n(96, 5)             1    (96, 5)   \n(97, 5)             1    (97, 5)   \n(98, 5)             1    (98, 5)   \n(100, 5)            1   (100, 5)   \n(103, 5)            1   (103, 5)   \n(108, 5)            1   (108, 5)   \n(111, 5)            1   (111, 5)   \n(113, 5)            1   (113, 5)   \n(114, 5)            1   (114, 5)   \n(120, 5)            2   (120, 5)   \n(123, 5)            1   (123, 5)   \n(127, 5)            2   (127, 5)   \n(131, 5)            1   (131, 5)   \n(135, 5)            1   (135, 5)   \n(145, 5)            1   (145, 5)   \n(151, 5)            1   (151, 5)   \n(154, 5)            1   (154, 5)   \n(157, 5)            1   (157, 5)   \n(161, 5)            1   (161, 5)   \n(169, 5)            1   (169, 5)   \n(178, 5)            2   (178, 5)   \n(179, 5)            2   (179, 5)   \n...               ...        ...   \n(2267, 5)           1  (2267, 5)   \n(2272, 5)           2  (2272, 5)   \n(2278, 5)           1  (2278, 5)   \n(2279, 5)           1  (2279, 5)   \n(2284, 5)           1  (2284, 5)   \n(2290, 5)           1  (2290, 5)   \n(2298, 5)           1  (2298, 5)   \n(2300, 5)           1  (2300, 5)   \n(2308, 5)           1  (2308, 5)   \n(2314, 5)           1  (2314, 5)   \n(2319, 5)           1  (2319, 5)   \n(2320, 5)           1  (2320, 5)   \n(2325, 5)           1  (2325, 5)   \n(2328, 5)           1  (2328, 5)   \n(2334, 5)           1  (2334, 5)   \n(2346, 5)           1  (2346, 5)   \n(2348, 5)           2  (2348, 5)   \n(2350, 5)           1  (2350, 5)   \n(2374, 5)           1  (2374, 5)   \n(2379, 5)           1  (2379, 5)   \n(2381, 5)           1  (2381, 5)   \n(2382, 5)           1  (2382, 5)   \n(2385, 5)           2  (2385, 5)   \n(2388, 5)           1  (2388, 5)   \n(2389, 5)           1  (2389, 5)   \n(2390, 5)           2  (2390, 5)   \n(2393, 5)           1  (2393, 5)   \n(2402, 5)           1  (2402, 5)   \n(2403, 5)           1  (2403, 5)   \n(2407, 5)           1  (2407, 5)   \n\n                                                 probability  \\\ncell                                                           \n(12, 5)                                 [0.5129718302458491]   \n(26, 5)                                 [0.5117321209240264]   \n(35, 5)                                 [0.5246889485427785]   \n(44, 5)             [0.5596671708080119, 0.5026945983566269]   \n(46, 5)             [0.5129718302458491, 0.5596671708080119]   \n(70, 5)                                 [0.5468403303422108]   \n(78, 5)    [0.5129718302458491, 0.5184749112477279, 0.502...   \n(79, 5)                                 [0.5468403303422108]   \n(96, 5)                                 [0.5246889485427785]   \n(97, 5)                                 [0.5246889485427785]   \n(98, 5)                                 [0.5117321209240264]   \n(100, 5)                                [0.5117321209240264]   \n(103, 5)                                [0.5117321209240264]   \n(108, 5)                                [0.5117321209240264]   \n(111, 5)                                [0.5117321209240264]   \n(113, 5)                                [0.5117321209240264]   \n(114, 5)                                [0.5117321209240264]   \n(120, 5)            [0.5596671708080119, 0.5026945983566269]   \n(123, 5)                                [0.5468403303422108]   \n(127, 5)             [0.5597543486907339, 0.573320543380727]   \n(131, 5)                                [0.5144249219590883]   \n(135, 5)                                [0.5129718302458491]   \n(145, 5)                                [0.5014541533464889]   \n(151, 5)                                [0.5014541533464889]   \n(154, 5)                                [0.5014541533464889]   \n(157, 5)                                [0.5129718302458491]   \n(161, 5)                                [0.5014541533464889]   \n(169, 5)                                [0.5026945983566269]   \n(178, 5)            [0.5026945983566269, 0.5694592785849029]   \n(179, 5)            [0.5596671708080119, 0.5694592785849029]   \n...                                                      ...   \n(2267, 5)                               [0.5117321209240264]   \n(2272, 5)           [0.5055083346175661, 0.5468403303422108]   \n(2278, 5)                               [0.5014541533464889]   \n(2279, 5)                               [0.5014541533464889]   \n(2284, 5)                               [0.5014541533464889]   \n(2290, 5)                               [0.5014541533464889]   \n(2298, 5)                               [0.5596671708080119]   \n(2300, 5)                               [0.5468403303422108]   \n(2308, 5)                               [0.5014541533464889]   \n(2314, 5)                               [0.5026945983566269]   \n(2319, 5)                               [0.5129718302458491]   \n(2320, 5)                               [0.5129718302458491]   \n(2325, 5)                               [0.5129718302458491]   \n(2328, 5)                               [0.5129718302458491]   \n(2334, 5)                               [0.5468403303422108]   \n(2346, 5)                               [0.5468403303422108]   \n(2348, 5)           [0.5596671708080119, 0.5129718302458491]   \n(2350, 5)                               [0.5055083346175661]   \n(2374, 5)                               [0.5468403303422108]   \n(2379, 5)                               [0.5129718302458491]   \n(2381, 5)                               [0.5596671708080119]   \n(2382, 5)                               [0.5596671708080119]   \n(2385, 5)           [0.5129718302458491, 0.5026945983566269]   \n(2388, 5)                               [0.5014541533464889]   \n(2389, 5)                               [0.5144249219590883]   \n(2390, 5)           [0.5129718302458491, 0.5468403303422108]   \n(2393, 5)                               [0.5129718302458491]   \n(2402, 5)                               [0.5014541533464889]   \n(2403, 5)                               [0.5014541533464889]   \n(2407, 5)                               [0.5014541533464889]   \n\n                     correction  \ncell                             \n(12, 5)                 [0.067]  \n(26, 5)                 [0.067]  \n(35, 5)                 [0.067]  \n(44, 5)           [0.05, 0.053]  \n(46, 5)           [0.067, 0.05]  \n(70, 5)                  [0.05]  \n(78, 5)    [0.067, 0.05, 0.053]  \n(79, 5)                  [0.05]  \n(96, 5)                 [0.067]  \n(97, 5)                 [0.067]  \n(98, 5)                 [0.067]  \n(100, 5)                [0.067]  \n(103, 5)                [0.067]  \n(108, 5)                [0.067]  \n(111, 5)                [0.067]  \n(113, 5)                [0.067]  \n(114, 5)                [0.067]  \n(120, 5)          [0.05, 0.053]  \n(123, 5)                 [0.07]  \n(127, 5)          [0.053, 0.04]  \n(131, 5)                [0.053]  \n(135, 5)                [0.067]  \n(145, 5)                [0.053]  \n(151, 5)                [0.053]  \n(154, 5)                [0.053]  \n(157, 5)                [0.067]  \n(161, 5)                [0.053]  \n(169, 5)                [0.053]  \n(178, 5)         [0.053, 0.051]  \n(179, 5)          [0.05, 0.051]  \n...                         ...  \n(2267, 5)               [0.067]  \n(2272, 5)         [0.055, 0.06]  \n(2278, 5)               [0.053]  \n(2279, 5)               [0.053]  \n(2284, 5)               [0.053]  \n(2290, 5)               [0.053]  \n(2298, 5)                [0.06]  \n(2300, 5)                [0.06]  \n(2308, 5)               [0.053]  \n(2314, 5)               [0.053]  \n(2319, 5)               [0.067]  \n(2320, 5)               [0.067]  \n(2325, 5)               [0.067]  \n(2328, 5)               [0.067]  \n(2334, 5)                [0.05]  \n(2346, 5)                [0.05]  \n(2348, 5)         [0.05, 0.067]  \n(2350, 5)                [0.05]  \n(2374, 5)                [0.05]  \n(2379, 5)               [0.067]  \n(2381, 5)                [0.05]  \n(2382, 5)                [0.05]  \n(2385, 5)        [0.067, 0.053]  \n(2388, 5)               [0.053]  \n(2389, 5)               [0.053]  \n(2390, 5)        [0.067, 0.045]  \n(2393, 5)               [0.067]  \n(2402, 5)               [0.053]  \n(2403, 5)               [0.053]  \n(2407, 5)               [0.053]  \n\n[371 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prediction</th>\n      <th>cell</th>\n      <th>probability</th>\n      <th>correction</th>\n    </tr>\n    <tr>\n      <th>cell</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>(12, 5)</th>\n      <td>1</td>\n      <td>(12, 5)</td>\n      <td>[0.5129718302458491]</td>\n      <td>[0.067]</td>\n    </tr>\n    <tr>\n      <th>(26, 5)</th>\n      <td>1</td>\n      <td>(26, 5)</td>\n      <td>[0.5117321209240264]</td>\n      <td>[0.067]</td>\n    </tr>\n    <tr>\n      <th>(35, 5)</th>\n      <td>1</td>\n      <td>(35, 5)</td>\n      <td>[0.5246889485427785]</td>\n      <td>[0.067]</td>\n    </tr>\n    <tr>\n      <th>(44, 5)</th>\n      <td>2</td>\n      <td>(44, 5)</td>\n      <td>[0.5596671708080119, 0.5026945983566269]</td>\n      <td>[0.05, 0.053]</td>\n    </tr>\n    <tr>\n      <th>(46, 5)</th>\n      <td>2</td>\n      <td>(46, 5)</td>\n      <td>[0.5129718302458491, 0.5596671708080119]</td>\n      <td>[0.067, 0.05]</td>\n    </tr>\n    <tr>\n      <th>(70, 5)</th>\n      <td>1</td>\n      <td>(70, 5)</td>\n      <td>[0.5468403303422108]</td>\n      <td>[0.05]</td>\n    </tr>\n    <tr>\n      <th>(78, 5)</th>\n      <td>3</td>\n      <td>(78, 5)</td>\n      <td>[0.5129718302458491, 0.5184749112477279, 0.502...</td>\n      <td>[0.067, 0.05, 0.053]</td>\n    </tr>\n    <tr>\n      <th>(79, 5)</th>\n      <td>1</td>\n      <td>(79, 5)</td>\n      <td>[0.5468403303422108]</td>\n      <td>[0.05]</td>\n    </tr>\n    <tr>\n      <th>(96, 5)</th>\n      <td>1</td>\n      <td>(96, 5)</td>\n      <td>[0.5246889485427785]</td>\n      <td>[0.067]</td>\n    </tr>\n    <tr>\n      <th>(97, 5)</th>\n      <td>1</td>\n      <td>(97, 5)</td>\n      <td>[0.5246889485427785]</td>\n      <td>[0.067]</td>\n    </tr>\n    <tr>\n      <th>(98, 5)</th>\n      <td>1</td>\n      <td>(98, 5)</td>\n      <td>[0.5117321209240264]</td>\n      <td>[0.067]</td>\n    </tr>\n    <tr>\n      <th>(100, 5)</th>\n      <td>1</td>\n      <td>(100, 5)</td>\n      <td>[0.5117321209240264]</td>\n      <td>[0.067]</td>\n    </tr>\n    <tr>\n      <th>(103, 5)</th>\n      <td>1</td>\n      <td>(103, 5)</td>\n      <td>[0.5117321209240264]</td>\n      <td>[0.067]</td>\n    </tr>\n    <tr>\n      <th>(108, 5)</th>\n      <td>1</td>\n      <td>(108, 5)</td>\n      <td>[0.5117321209240264]</td>\n      <td>[0.067]</td>\n    </tr>\n    <tr>\n      <th>(111, 5)</th>\n      <td>1</td>\n      <td>(111, 5)</td>\n      <td>[0.5117321209240264]</td>\n      <td>[0.067]</td>\n    </tr>\n    <tr>\n      <th>(113, 5)</th>\n      <td>1</td>\n      <td>(113, 5)</td>\n      <td>[0.5117321209240264]</td>\n      <td>[0.067]</td>\n    </tr>\n    <tr>\n      <th>(114, 5)</th>\n      <td>1</td>\n      <td>(114, 5)</td>\n      <td>[0.5117321209240264]</td>\n      <td>[0.067]</td>\n    </tr>\n    <tr>\n      <th>(120, 5)</th>\n      <td>2</td>\n      <td>(120, 5)</td>\n      <td>[0.5596671708080119, 0.5026945983566269]</td>\n      <td>[0.05, 0.053]</td>\n    </tr>\n    <tr>\n      <th>(123, 5)</th>\n      <td>1</td>\n      <td>(123, 5)</td>\n      <td>[0.5468403303422108]</td>\n      <td>[0.07]</td>\n    </tr>\n    <tr>\n      <th>(127, 5)</th>\n      <td>2</td>\n      <td>(127, 5)</td>\n      <td>[0.5597543486907339, 0.573320543380727]</td>\n      <td>[0.053, 0.04]</td>\n    </tr>\n    <tr>\n      <th>(131, 5)</th>\n      <td>1</td>\n      <td>(131, 5)</td>\n      <td>[0.5144249219590883]</td>\n      <td>[0.053]</td>\n    </tr>\n    <tr>\n      <th>(135, 5)</th>\n      <td>1</td>\n      <td>(135, 5)</td>\n      <td>[0.5129718302458491]</td>\n      <td>[0.067]</td>\n    </tr>\n    <tr>\n      <th>(145, 5)</th>\n      <td>1</td>\n      <td>(145, 5)</td>\n      <td>[0.5014541533464889]</td>\n      <td>[0.053]</td>\n    </tr>\n    <tr>\n      <th>(151, 5)</th>\n      <td>1</td>\n      <td>(151, 5)</td>\n      <td>[0.5014541533464889]</td>\n      <td>[0.053]</td>\n    </tr>\n    <tr>\n      <th>(154, 5)</th>\n      <td>1</td>\n      <td>(154, 5)</td>\n      <td>[0.5014541533464889]</td>\n      <td>[0.053]</td>\n    </tr>\n    <tr>\n      <th>(157, 5)</th>\n      <td>1</td>\n      <td>(157, 5)</td>\n      <td>[0.5129718302458491]</td>\n      <td>[0.067]</td>\n    </tr>\n    <tr>\n      <th>(161, 5)</th>\n      <td>1</td>\n      <td>(161, 5)</td>\n      <td>[0.5014541533464889]</td>\n      <td>[0.053]</td>\n    </tr>\n    <tr>\n      <th>(169, 5)</th>\n      <td>1</td>\n      <td>(169, 5)</td>\n      <td>[0.5026945983566269]</td>\n      <td>[0.053]</td>\n    </tr>\n    <tr>\n      <th>(178, 5)</th>\n      <td>2</td>\n      <td>(178, 5)</td>\n      <td>[0.5026945983566269, 0.5694592785849029]</td>\n      <td>[0.053, 0.051]</td>\n    </tr>\n    <tr>\n      <th>(179, 5)</th>\n      <td>2</td>\n      <td>(179, 5)</td>\n      <td>[0.5596671708080119, 0.5694592785849029]</td>\n      <td>[0.05, 0.051]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>(2267, 5)</th>\n      <td>1</td>\n      <td>(2267, 5)</td>\n      <td>[0.5117321209240264]</td>\n      <td>[0.067]</td>\n    </tr>\n    <tr>\n      <th>(2272, 5)</th>\n      <td>2</td>\n      <td>(2272, 5)</td>\n      <td>[0.5055083346175661, 0.5468403303422108]</td>\n      <td>[0.055, 0.06]</td>\n    </tr>\n    <tr>\n      <th>(2278, 5)</th>\n      <td>1</td>\n      <td>(2278, 5)</td>\n      <td>[0.5014541533464889]</td>\n      <td>[0.053]</td>\n    </tr>\n    <tr>\n      <th>(2279, 5)</th>\n      <td>1</td>\n      <td>(2279, 5)</td>\n      <td>[0.5014541533464889]</td>\n      <td>[0.053]</td>\n    </tr>\n    <tr>\n      <th>(2284, 5)</th>\n      <td>1</td>\n      <td>(2284, 5)</td>\n      <td>[0.5014541533464889]</td>\n      <td>[0.053]</td>\n    </tr>\n    <tr>\n      <th>(2290, 5)</th>\n      <td>1</td>\n      <td>(2290, 5)</td>\n      <td>[0.5014541533464889]</td>\n      <td>[0.053]</td>\n    </tr>\n    <tr>\n      <th>(2298, 5)</th>\n      <td>1</td>\n      <td>(2298, 5)</td>\n      <td>[0.5596671708080119]</td>\n      <td>[0.06]</td>\n    </tr>\n    <tr>\n      <th>(2300, 5)</th>\n      <td>1</td>\n      <td>(2300, 5)</td>\n      <td>[0.5468403303422108]</td>\n      <td>[0.06]</td>\n    </tr>\n    <tr>\n      <th>(2308, 5)</th>\n      <td>1</td>\n      <td>(2308, 5)</td>\n      <td>[0.5014541533464889]</td>\n      <td>[0.053]</td>\n    </tr>\n    <tr>\n      <th>(2314, 5)</th>\n      <td>1</td>\n      <td>(2314, 5)</td>\n      <td>[0.5026945983566269]</td>\n      <td>[0.053]</td>\n    </tr>\n    <tr>\n      <th>(2319, 5)</th>\n      <td>1</td>\n      <td>(2319, 5)</td>\n      <td>[0.5129718302458491]</td>\n      <td>[0.067]</td>\n    </tr>\n    <tr>\n      <th>(2320, 5)</th>\n      <td>1</td>\n      <td>(2320, 5)</td>\n      <td>[0.5129718302458491]</td>\n      <td>[0.067]</td>\n    </tr>\n    <tr>\n      <th>(2325, 5)</th>\n      <td>1</td>\n      <td>(2325, 5)</td>\n      <td>[0.5129718302458491]</td>\n      <td>[0.067]</td>\n    </tr>\n    <tr>\n      <th>(2328, 5)</th>\n      <td>1</td>\n      <td>(2328, 5)</td>\n      <td>[0.5129718302458491]</td>\n      <td>[0.067]</td>\n    </tr>\n    <tr>\n      <th>(2334, 5)</th>\n      <td>1</td>\n      <td>(2334, 5)</td>\n      <td>[0.5468403303422108]</td>\n      <td>[0.05]</td>\n    </tr>\n    <tr>\n      <th>(2346, 5)</th>\n      <td>1</td>\n      <td>(2346, 5)</td>\n      <td>[0.5468403303422108]</td>\n      <td>[0.05]</td>\n    </tr>\n    <tr>\n      <th>(2348, 5)</th>\n      <td>2</td>\n      <td>(2348, 5)</td>\n      <td>[0.5596671708080119, 0.5129718302458491]</td>\n      <td>[0.05, 0.067]</td>\n    </tr>\n    <tr>\n      <th>(2350, 5)</th>\n      <td>1</td>\n      <td>(2350, 5)</td>\n      <td>[0.5055083346175661]</td>\n      <td>[0.05]</td>\n    </tr>\n    <tr>\n      <th>(2374, 5)</th>\n      <td>1</td>\n      <td>(2374, 5)</td>\n      <td>[0.5468403303422108]</td>\n      <td>[0.05]</td>\n    </tr>\n    <tr>\n      <th>(2379, 5)</th>\n      <td>1</td>\n      <td>(2379, 5)</td>\n      <td>[0.5129718302458491]</td>\n      <td>[0.067]</td>\n    </tr>\n    <tr>\n      <th>(2381, 5)</th>\n      <td>1</td>\n      <td>(2381, 5)</td>\n      <td>[0.5596671708080119]</td>\n      <td>[0.05]</td>\n    </tr>\n    <tr>\n      <th>(2382, 5)</th>\n      <td>1</td>\n      <td>(2382, 5)</td>\n      <td>[0.5596671708080119]</td>\n      <td>[0.05]</td>\n    </tr>\n    <tr>\n      <th>(2385, 5)</th>\n      <td>2</td>\n      <td>(2385, 5)</td>\n      <td>[0.5129718302458491, 0.5026945983566269]</td>\n      <td>[0.067, 0.053]</td>\n    </tr>\n    <tr>\n      <th>(2388, 5)</th>\n      <td>1</td>\n      <td>(2388, 5)</td>\n      <td>[0.5014541533464889]</td>\n      <td>[0.053]</td>\n    </tr>\n    <tr>\n      <th>(2389, 5)</th>\n      <td>1</td>\n      <td>(2389, 5)</td>\n      <td>[0.5144249219590883]</td>\n      <td>[0.053]</td>\n    </tr>\n    <tr>\n      <th>(2390, 5)</th>\n      <td>2</td>\n      <td>(2390, 5)</td>\n      <td>[0.5129718302458491, 0.5468403303422108]</td>\n      <td>[0.067, 0.045]</td>\n    </tr>\n    <tr>\n      <th>(2393, 5)</th>\n      <td>1</td>\n      <td>(2393, 5)</td>\n      <td>[0.5129718302458491]</td>\n      <td>[0.067]</td>\n    </tr>\n    <tr>\n      <th>(2402, 5)</th>\n      <td>1</td>\n      <td>(2402, 5)</td>\n      <td>[0.5014541533464889]</td>\n      <td>[0.053]</td>\n    </tr>\n    <tr>\n      <th>(2403, 5)</th>\n      <td>1</td>\n      <td>(2403, 5)</td>\n      <td>[0.5014541533464889]</td>\n      <td>[0.053]</td>\n    </tr>\n    <tr>\n      <th>(2407, 5)</th>\n      <td>1</td>\n      <td>(2407, 5)</td>\n      <td>[0.5014541533464889]</td>\n      <td>[0.053]</td>\n    </tr>\n  </tbody>\n</table>\n<p>371 rows  4 columns</p>\n</div>"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated = df[df[\"prediction\"] == 1].groupby(\"cell\").agg({\"prediction\": \"sum\",\n",
    "                                     \"cell\": \"first\",\n",
    "                                     \"probability\": list,\n",
    "                                     \"correction\": list})\n",
    "aggregated"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(59, 5)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-50-17e902cda7f7>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Compare correction and actual\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mcell\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;36m59\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0md\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcorrected_cells\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcell\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mactual_errors\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcell\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: (59, 5)"
     ]
    }
   ],
   "source": [
    "# Compare correction and actual\n",
    "cell = (59,5)\n",
    "print(d.corrected_cells[cell])\n",
    "print(actual_errors[cell])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "           cell  confidence  detection_correct  correct\n0       (90, 4)    1.000000               True     True\n1      (192, 4)    1.000000               True     True\n2      (323, 4)    1.000000               True     True\n3      (388, 4)    1.000000               True     True\n4      (442, 4)    1.000000               True     True\n5      (553, 4)    1.000000               True     True\n6     (1000, 4)    1.000000               True     True\n7     (1039, 4)    1.000000               True     True\n8     (1121, 4)    1.000000               True     True\n9     (1185, 4)    1.000000               True     True\n10    (1579, 4)    1.000000               True     True\n11    (1666, 4)    1.000000               True     True\n12    (1955, 4)    1.000000               True     True\n13    (1969, 4)    1.000000               True     True\n14    (2221, 4)    1.000000               True     True\n15    (2251, 4)    1.000000               True     True\n16    (2260, 4)    1.000000               True     True\n17    (2391, 4)    1.000000               True     True\n18    (2394, 4)    1.000000               True     True\n19    (2395, 4)    1.000000               True     True\n20       (0, 4)    0.999995               True    False\n21       (1, 4)    1.000000               True     True\n22       (3, 4)    0.794066               True     True\n23       (5, 4)    0.794066               True     True\n24       (6, 4)    1.000000               True     True\n25       (7, 4)    1.000000               True     True\n26       (8, 4)    1.000000               True     True\n27       (9, 4)    1.000000               True     True\n28      (10, 4)    1.000000               True     True\n29      (11, 4)    1.000000               True     True\n...         ...         ...                ...      ...\n4274  (1307, 5)    1.000000               True     True\n4275  (1333, 5)    1.000000               True     True\n4276  (1466, 5)    1.000000               True     True\n4277  (1469, 5)    1.000000               True     True\n4278  (1599, 5)    1.000000               True     True\n4279  (1706, 5)    1.000000               True     True\n4280  (1727, 5)    1.000000               True     True\n4281  (1790, 5)    1.000000               True     True\n4282  (1794, 5)    1.000000               True     True\n4283  (1814, 5)    1.000000               True     True\n4284  (1817, 5)    1.000000               True     True\n4285  (1832, 5)    1.000000               True     True\n4286  (1898, 5)    1.000000               True     True\n4287  (1941, 5)    1.000000               True     True\n4288  (2056, 5)    1.000000               True     True\n4289  (2063, 5)    1.000000               True     True\n4290  (2112, 5)    1.000000               True     True\n4291  (2133, 5)    1.000000               True     True\n4292  (2147, 5)    1.000000               True     True\n4293  (2174, 5)    1.000000               True     True\n4294  (2212, 5)    1.000000               True     True\n4295  (2250, 5)    1.000000               True     True\n4296  (2261, 5)    1.000000               True     True\n4297  (2378, 5)    1.000000               True     True\n4298   (309, 9)    1.000000               True    False\n4299   (836, 9)    1.000000               True    False\n4300  (2135, 9)    1.000000               True    False\n4301   (916, 5)    1.000000               True     True\n4302  (1451, 5)    1.000000               True     True\n4303  (1669, 5)    1.000000               True     True\n\n[4304 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cell</th>\n      <th>confidence</th>\n      <th>detection_correct</th>\n      <th>correct</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>(90, 4)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(192, 4)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>(323, 4)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>(388, 4)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>(442, 4)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>(553, 4)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>(1000, 4)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>(1039, 4)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>(1121, 4)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>(1185, 4)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>(1579, 4)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>(1666, 4)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>(1955, 4)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>(1969, 4)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>(2221, 4)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>(2251, 4)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>(2260, 4)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>(2391, 4)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>(2394, 4)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>(2395, 4)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>(0, 4)</td>\n      <td>0.999995</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>(1, 4)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>(3, 4)</td>\n      <td>0.794066</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>(5, 4)</td>\n      <td>0.794066</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>(6, 4)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>(7, 4)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>(8, 4)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>(9, 4)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>(10, 4)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>(11, 4)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4274</th>\n      <td>(1307, 5)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4275</th>\n      <td>(1333, 5)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4276</th>\n      <td>(1466, 5)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4277</th>\n      <td>(1469, 5)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4278</th>\n      <td>(1599, 5)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4279</th>\n      <td>(1706, 5)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4280</th>\n      <td>(1727, 5)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4281</th>\n      <td>(1790, 5)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4282</th>\n      <td>(1794, 5)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4283</th>\n      <td>(1814, 5)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4284</th>\n      <td>(1817, 5)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4285</th>\n      <td>(1832, 5)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4286</th>\n      <td>(1898, 5)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4287</th>\n      <td>(1941, 5)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4288</th>\n      <td>(2056, 5)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4289</th>\n      <td>(2063, 5)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4290</th>\n      <td>(2112, 5)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4291</th>\n      <td>(2133, 5)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4292</th>\n      <td>(2147, 5)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4293</th>\n      <td>(2174, 5)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4294</th>\n      <td>(2212, 5)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4295</th>\n      <td>(2250, 5)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4296</th>\n      <td>(2261, 5)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4297</th>\n      <td>(2378, 5)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4298</th>\n      <td>(309, 9)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4299</th>\n      <td>(836, 9)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4300</th>\n      <td>(2135, 9)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4301</th>\n      <td>(916, 5)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4302</th>\n      <td>(1451, 5)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4303</th>\n      <td>(1669, 5)</td>\n      <td>1.000000</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n<p>4304 rows  4 columns</p>\n</div>"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction_confidence_df = analysis_utilities.get_correction_confidence_df(d, actual_errors)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of confidences for wrong (False) and correct (True) corrections:\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "analysis_utilities.correction_confidence_distributions(correction_confidence_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empirical probability of a correction being wrong given its confidence:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/anaconda3/envs/raha/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "importlib.reload(analysis_utilities)\n",
    "\n",
    "f = analysis_utilities.correction_correctness_by_confidence(correction_confidence_df)\n",
    "f.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Analyze steps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "            cell correction  prediction  probability\n143      (12, 5)      0.067           1     0.512972\n307      (26, 5)      0.067           1     0.511732\n564      (35, 5)      0.067           1     0.524689\n690      (44, 5)       0.05           1     0.559667\n704      (44, 5)      0.053           1     0.502695\n771      (46, 5)      0.067           1     0.512972\n774      (46, 5)       0.05           1     0.559667\n1045     (70, 5)       0.05           1     0.546840\n1109     (78, 5)      0.067           1     0.512972\n1114     (78, 5)       0.05           1     0.518475\n1116     (78, 5)      0.053           1     0.502695\n1180     (79, 5)       0.05           1     0.546840\n1254     (96, 5)      0.067           1     0.524689\n1323     (97, 5)      0.067           1     0.524689\n1413     (98, 5)      0.067           1     0.511732\n1476    (100, 5)      0.067           1     0.511732\n1540    (103, 5)      0.067           1     0.511732\n1595    (108, 5)      0.067           1     0.511732\n1664    (111, 5)      0.067           1     0.511732\n1752    (113, 5)      0.067           1     0.511732\n1820    (114, 5)      0.067           1     0.511732\n1867    (120, 5)       0.05           1     0.559667\n1868    (120, 5)      0.053           1     0.502695\n2016    (123, 5)       0.07           1     0.546840\n2071    (127, 5)      0.053           1     0.559754\n2088    (127, 5)       0.04           1     0.573321\n2144    (131, 5)      0.053           1     0.514425\n2213    (135, 5)      0.067           1     0.512972\n2317    (145, 5)      0.053           1     0.501454\n2417    (151, 5)      0.053           1     0.501454\n...          ...        ...         ...          ...\n44671  (2279, 5)      0.053           1     0.501454\n44742  (2284, 5)      0.053           1     0.501454\n44810  (2290, 5)      0.053           1     0.501454\n45008  (2298, 5)       0.06           1     0.559667\n45062  (2300, 5)       0.06           1     0.546840\n45142  (2308, 5)      0.053           1     0.501454\n45209  (2314, 5)      0.053           1     0.502695\n45345  (2319, 5)      0.067           1     0.512972\n45414  (2320, 5)      0.067           1     0.512972\n45552  (2325, 5)      0.067           1     0.512972\n45690  (2328, 5)      0.067           1     0.512972\n45760  (2334, 5)       0.05           1     0.546840\n45892  (2346, 5)       0.05           1     0.546840\n45958  (2348, 5)       0.05           1     0.559667\n45966  (2348, 5)      0.067           1     0.512972\n46025  (2350, 5)       0.05           1     0.505508\n46579  (2374, 5)       0.05           1     0.546840\n46725  (2379, 5)      0.067           1     0.512972\n46788  (2381, 5)       0.05           1     0.559667\n46855  (2382, 5)       0.05           1     0.559667\n46925  (2385, 5)      0.067           1     0.512972\n46932  (2385, 5)      0.053           1     0.502695\n47028  (2388, 5)      0.053           1     0.501454\n47072  (2389, 5)      0.053           1     0.514425\n47139  (2390, 5)      0.067           1     0.512972\n47149  (2390, 5)      0.045           1     0.546840\n47208  (2393, 5)      0.067           1     0.512972\n47362  (2402, 5)      0.053           1     0.501454\n47430  (2403, 5)      0.053           1     0.501454\n47496  (2407, 5)      0.053           1     0.501454\n\n[501 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cell</th>\n      <th>correction</th>\n      <th>prediction</th>\n      <th>probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>143</th>\n      <td>(12, 5)</td>\n      <td>0.067</td>\n      <td>1</td>\n      <td>0.512972</td>\n    </tr>\n    <tr>\n      <th>307</th>\n      <td>(26, 5)</td>\n      <td>0.067</td>\n      <td>1</td>\n      <td>0.511732</td>\n    </tr>\n    <tr>\n      <th>564</th>\n      <td>(35, 5)</td>\n      <td>0.067</td>\n      <td>1</td>\n      <td>0.524689</td>\n    </tr>\n    <tr>\n      <th>690</th>\n      <td>(44, 5)</td>\n      <td>0.05</td>\n      <td>1</td>\n      <td>0.559667</td>\n    </tr>\n    <tr>\n      <th>704</th>\n      <td>(44, 5)</td>\n      <td>0.053</td>\n      <td>1</td>\n      <td>0.502695</td>\n    </tr>\n    <tr>\n      <th>771</th>\n      <td>(46, 5)</td>\n      <td>0.067</td>\n      <td>1</td>\n      <td>0.512972</td>\n    </tr>\n    <tr>\n      <th>774</th>\n      <td>(46, 5)</td>\n      <td>0.05</td>\n      <td>1</td>\n      <td>0.559667</td>\n    </tr>\n    <tr>\n      <th>1045</th>\n      <td>(70, 5)</td>\n      <td>0.05</td>\n      <td>1</td>\n      <td>0.546840</td>\n    </tr>\n    <tr>\n      <th>1109</th>\n      <td>(78, 5)</td>\n      <td>0.067</td>\n      <td>1</td>\n      <td>0.512972</td>\n    </tr>\n    <tr>\n      <th>1114</th>\n      <td>(78, 5)</td>\n      <td>0.05</td>\n      <td>1</td>\n      <td>0.518475</td>\n    </tr>\n    <tr>\n      <th>1116</th>\n      <td>(78, 5)</td>\n      <td>0.053</td>\n      <td>1</td>\n      <td>0.502695</td>\n    </tr>\n    <tr>\n      <th>1180</th>\n      <td>(79, 5)</td>\n      <td>0.05</td>\n      <td>1</td>\n      <td>0.546840</td>\n    </tr>\n    <tr>\n      <th>1254</th>\n      <td>(96, 5)</td>\n      <td>0.067</td>\n      <td>1</td>\n      <td>0.524689</td>\n    </tr>\n    <tr>\n      <th>1323</th>\n      <td>(97, 5)</td>\n      <td>0.067</td>\n      <td>1</td>\n      <td>0.524689</td>\n    </tr>\n    <tr>\n      <th>1413</th>\n      <td>(98, 5)</td>\n      <td>0.067</td>\n      <td>1</td>\n      <td>0.511732</td>\n    </tr>\n    <tr>\n      <th>1476</th>\n      <td>(100, 5)</td>\n      <td>0.067</td>\n      <td>1</td>\n      <td>0.511732</td>\n    </tr>\n    <tr>\n      <th>1540</th>\n      <td>(103, 5)</td>\n      <td>0.067</td>\n      <td>1</td>\n      <td>0.511732</td>\n    </tr>\n    <tr>\n      <th>1595</th>\n      <td>(108, 5)</td>\n      <td>0.067</td>\n      <td>1</td>\n      <td>0.511732</td>\n    </tr>\n    <tr>\n      <th>1664</th>\n      <td>(111, 5)</td>\n      <td>0.067</td>\n      <td>1</td>\n      <td>0.511732</td>\n    </tr>\n    <tr>\n      <th>1752</th>\n      <td>(113, 5)</td>\n      <td>0.067</td>\n      <td>1</td>\n      <td>0.511732</td>\n    </tr>\n    <tr>\n      <th>1820</th>\n      <td>(114, 5)</td>\n      <td>0.067</td>\n      <td>1</td>\n      <td>0.511732</td>\n    </tr>\n    <tr>\n      <th>1867</th>\n      <td>(120, 5)</td>\n      <td>0.05</td>\n      <td>1</td>\n      <td>0.559667</td>\n    </tr>\n    <tr>\n      <th>1868</th>\n      <td>(120, 5)</td>\n      <td>0.053</td>\n      <td>1</td>\n      <td>0.502695</td>\n    </tr>\n    <tr>\n      <th>2016</th>\n      <td>(123, 5)</td>\n      <td>0.07</td>\n      <td>1</td>\n      <td>0.546840</td>\n    </tr>\n    <tr>\n      <th>2071</th>\n      <td>(127, 5)</td>\n      <td>0.053</td>\n      <td>1</td>\n      <td>0.559754</td>\n    </tr>\n    <tr>\n      <th>2088</th>\n      <td>(127, 5)</td>\n      <td>0.04</td>\n      <td>1</td>\n      <td>0.573321</td>\n    </tr>\n    <tr>\n      <th>2144</th>\n      <td>(131, 5)</td>\n      <td>0.053</td>\n      <td>1</td>\n      <td>0.514425</td>\n    </tr>\n    <tr>\n      <th>2213</th>\n      <td>(135, 5)</td>\n      <td>0.067</td>\n      <td>1</td>\n      <td>0.512972</td>\n    </tr>\n    <tr>\n      <th>2317</th>\n      <td>(145, 5)</td>\n      <td>0.053</td>\n      <td>1</td>\n      <td>0.501454</td>\n    </tr>\n    <tr>\n      <th>2417</th>\n      <td>(151, 5)</td>\n      <td>0.053</td>\n      <td>1</td>\n      <td>0.501454</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>44671</th>\n      <td>(2279, 5)</td>\n      <td>0.053</td>\n      <td>1</td>\n      <td>0.501454</td>\n    </tr>\n    <tr>\n      <th>44742</th>\n      <td>(2284, 5)</td>\n      <td>0.053</td>\n      <td>1</td>\n      <td>0.501454</td>\n    </tr>\n    <tr>\n      <th>44810</th>\n      <td>(2290, 5)</td>\n      <td>0.053</td>\n      <td>1</td>\n      <td>0.501454</td>\n    </tr>\n    <tr>\n      <th>45008</th>\n      <td>(2298, 5)</td>\n      <td>0.06</td>\n      <td>1</td>\n      <td>0.559667</td>\n    </tr>\n    <tr>\n      <th>45062</th>\n      <td>(2300, 5)</td>\n      <td>0.06</td>\n      <td>1</td>\n      <td>0.546840</td>\n    </tr>\n    <tr>\n      <th>45142</th>\n      <td>(2308, 5)</td>\n      <td>0.053</td>\n      <td>1</td>\n      <td>0.501454</td>\n    </tr>\n    <tr>\n      <th>45209</th>\n      <td>(2314, 5)</td>\n      <td>0.053</td>\n      <td>1</td>\n      <td>0.502695</td>\n    </tr>\n    <tr>\n      <th>45345</th>\n      <td>(2319, 5)</td>\n      <td>0.067</td>\n      <td>1</td>\n      <td>0.512972</td>\n    </tr>\n    <tr>\n      <th>45414</th>\n      <td>(2320, 5)</td>\n      <td>0.067</td>\n      <td>1</td>\n      <td>0.512972</td>\n    </tr>\n    <tr>\n      <th>45552</th>\n      <td>(2325, 5)</td>\n      <td>0.067</td>\n      <td>1</td>\n      <td>0.512972</td>\n    </tr>\n    <tr>\n      <th>45690</th>\n      <td>(2328, 5)</td>\n      <td>0.067</td>\n      <td>1</td>\n      <td>0.512972</td>\n    </tr>\n    <tr>\n      <th>45760</th>\n      <td>(2334, 5)</td>\n      <td>0.05</td>\n      <td>1</td>\n      <td>0.546840</td>\n    </tr>\n    <tr>\n      <th>45892</th>\n      <td>(2346, 5)</td>\n      <td>0.05</td>\n      <td>1</td>\n      <td>0.546840</td>\n    </tr>\n    <tr>\n      <th>45958</th>\n      <td>(2348, 5)</td>\n      <td>0.05</td>\n      <td>1</td>\n      <td>0.559667</td>\n    </tr>\n    <tr>\n      <th>45966</th>\n      <td>(2348, 5)</td>\n      <td>0.067</td>\n      <td>1</td>\n      <td>0.512972</td>\n    </tr>\n    <tr>\n      <th>46025</th>\n      <td>(2350, 5)</td>\n      <td>0.05</td>\n      <td>1</td>\n      <td>0.505508</td>\n    </tr>\n    <tr>\n      <th>46579</th>\n      <td>(2374, 5)</td>\n      <td>0.05</td>\n      <td>1</td>\n      <td>0.546840</td>\n    </tr>\n    <tr>\n      <th>46725</th>\n      <td>(2379, 5)</td>\n      <td>0.067</td>\n      <td>1</td>\n      <td>0.512972</td>\n    </tr>\n    <tr>\n      <th>46788</th>\n      <td>(2381, 5)</td>\n      <td>0.05</td>\n      <td>1</td>\n      <td>0.559667</td>\n    </tr>\n    <tr>\n      <th>46855</th>\n      <td>(2382, 5)</td>\n      <td>0.05</td>\n      <td>1</td>\n      <td>0.559667</td>\n    </tr>\n    <tr>\n      <th>46925</th>\n      <td>(2385, 5)</td>\n      <td>0.067</td>\n      <td>1</td>\n      <td>0.512972</td>\n    </tr>\n    <tr>\n      <th>46932</th>\n      <td>(2385, 5)</td>\n      <td>0.053</td>\n      <td>1</td>\n      <td>0.502695</td>\n    </tr>\n    <tr>\n      <th>47028</th>\n      <td>(2388, 5)</td>\n      <td>0.053</td>\n      <td>1</td>\n      <td>0.501454</td>\n    </tr>\n    <tr>\n      <th>47072</th>\n      <td>(2389, 5)</td>\n      <td>0.053</td>\n      <td>1</td>\n      <td>0.514425</td>\n    </tr>\n    <tr>\n      <th>47139</th>\n      <td>(2390, 5)</td>\n      <td>0.067</td>\n      <td>1</td>\n      <td>0.512972</td>\n    </tr>\n    <tr>\n      <th>47149</th>\n      <td>(2390, 5)</td>\n      <td>0.045</td>\n      <td>1</td>\n      <td>0.546840</td>\n    </tr>\n    <tr>\n      <th>47208</th>\n      <td>(2393, 5)</td>\n      <td>0.067</td>\n      <td>1</td>\n      <td>0.512972</td>\n    </tr>\n    <tr>\n      <th>47362</th>\n      <td>(2402, 5)</td>\n      <td>0.053</td>\n      <td>1</td>\n      <td>0.501454</td>\n    </tr>\n    <tr>\n      <th>47430</th>\n      <td>(2403, 5)</td>\n      <td>0.053</td>\n      <td>1</td>\n      <td>0.501454</td>\n    </tr>\n    <tr>\n      <th>47496</th>\n      <td>(2407, 5)</td>\n      <td>0.053</td>\n      <td>1</td>\n      <td>0.501454</td>\n    </tr>\n  </tbody>\n</table>\n<p>501 rows  4 columns</p>\n</div>"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction of step 0 for column 5\n",
    "d.correction_prediction_dfs[0][5][d.correction_prediction_dfs[0][5][\"prediction\"] == 1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1080x864 with 20 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(4, 5, sharex=\"all\", sharey=\"row\", figsize=(15,12))\n",
    "\n",
    "for row in range(4):\n",
    "    for column in range(5):\n",
    "        df = d.correction_prediction_dfs[row * 5 + column][5]\n",
    "        df[\"probability\"].hist(bins=np.linspace(0.0, 1.0, 21), ax=axs[row][column])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}